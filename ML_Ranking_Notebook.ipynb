{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6cd1f2bb",
      "metadata": {
        "id": "6cd1f2bb"
      },
      "source": [
        "# ML-Driven Search Ranking Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "65048c4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65048c4d",
        "outputId": "6806997c-8d7a-4be9-abc0-f9b8efd05cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading mq2008_sample.txt...\n",
            "Download complete.\n",
            "Dataset loaded with 3005 documents across 2 queries.\n",
            "Number of features: 217\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6155\n",
            "[LightGBM] [Info] Number of data points in the train set: 2003, number of used features: 214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:86: UserWarning: The groups parameter is ignored by KFold\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004108 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6142\n",
            "[LightGBM] [Info] Number of data points in the train set: 2003, number of used features: 213\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004031 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6151\n",
            "[LightGBM] [Info] Number of data points in the train set: 2004, number of used features: 212\n",
            "\n",
            "Model training complete for all folds.\n",
            "\n",
            "Making predictions on a new, hypothetical query...\n",
            "\n",
            "Predicted rankings for the new query:\n",
            "   PredictedScore\n",
            "0       -3.820940\n",
            "2       -5.000245\n",
            "1       -5.146782\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pandas lightgbm scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.model_selection import KFold\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "\n",
        "def download_data(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "def load_svmlight_file(file_path):\n",
        "\n",
        "    data = []\n",
        "    qids = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' ')\n",
        "            relevance_score = int(parts[0])\n",
        "\n",
        "\n",
        "            query_id_str = parts[1].split(':')[1]\n",
        "            query_id = int(float(query_id_str))\n",
        "\n",
        "\n",
        "\n",
        "            features = {int(p.split(':')[0]): float(p.split(':')[1]) for p in parts[2:]}\n",
        "\n",
        "            data.append((query_id, relevance_score, features))\n",
        "\n",
        "\n",
        "    df_data = []\n",
        "    for qid, rel_score, feats in data:\n",
        "        row = {'QueryId': qid, 'RelScore': rel_score}\n",
        "        row.update(feats)\n",
        "        df_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(df_data)\n",
        "    df = df.fillna(0)\n",
        "\n",
        "\n",
        "    df = df.sort_values(by='QueryId').reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "data_url = \"https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/lambdarank/rank.train\"\n",
        "file_name = \"mq2008_sample.txt\"\n",
        "download_data(data_url, file_name)\n",
        "\n",
        "df = load_svmlight_file(file_name)\n",
        "\n",
        "\n",
        "X = df.drop(['QueryId', 'RelScore'], axis=1)\n",
        "y = df['RelScore']\n",
        "qids = df['QueryId']\n",
        "groups = df.groupby('QueryId').size().to_numpy()\n",
        "\n",
        "print(f\"Dataset loaded with {len(df)} documents across {len(groups)} queries.\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "\n",
        "\n",
        "k_fold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "models = []\n",
        "\n",
        "print(\"\\nStarting 3-fold cross-validation...\")\n",
        "\n",
        "for train_index, test_index in k_fold.split(X, y, groups=qids):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "\n",
        "    groups_train = X_train.groupby(qids.iloc[train_index]).size().to_numpy()\n",
        "\n",
        "    ranker = LGBMRanker(\n",
        "        objective=\"lambdarank\",\n",
        "        metric=\"ndcg\",\n",
        "        n_estimators=50,\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "\n",
        "    ranker.fit(X_train, y_train, group=groups_train)\n",
        "    models.append(ranker)\n",
        "\n",
        "print(\"\\nModel training complete for all folds.\")\n",
        "\n",
        "\n",
        "print(\"\\nMaking predictions on a new, hypothetical query...\")\n",
        "\n",
        "\n",
        "num_features = X.shape[1]\n",
        "new_query_data = pd.DataFrame(np.random.rand(3, num_features), columns=X.columns)\n",
        "\n",
        "\n",
        "predictions = models[0].predict(new_query_data)\n",
        "new_query_data['PredictedScore'] = predictions\n",
        "ranked_results = new_query_data.sort_values(by='PredictedScore', ascending=False)\n",
        "\n",
        "print(\"\\nPredicted rankings for the new query:\")\n",
        "print(ranked_results[['PredictedScore']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}