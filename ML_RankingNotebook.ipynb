{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6cd1f2bb",
      "metadata": {
        "id": "6cd1f2bb"
      },
      "source": [
        "# ML-Driven Search Ranking Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "65048c4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65048c4d",
        "outputId": "c488d135-2f60-4150-f448-59e323ea8ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 3005 documents across 2 queries.\n",
            "Number of features: 217\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004016 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6155\n",
            "[LightGBM] [Info] Number of data points in the train set: 2003, number of used features: 214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:86: UserWarning: The groups parameter is ignored by KFold\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003953 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6142\n",
            "[LightGBM] [Info] Number of data points in the train set: 2003, number of used features: 213\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003885 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6151\n",
            "[LightGBM] [Info] Number of data points in the train set: 2004, number of used features: 212\n",
            "\n",
            "Model training complete for all folds.\n",
            "\n",
            "Making predictions on a new, hypothetical query...\n",
            "\n",
            "Predicted rankings for the new query:\n",
            "   PredictedScore\n",
            "0       -3.220023\n",
            "2       -3.702575\n",
            "1       -4.060316\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "# pip install pandas lightgbm scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRanker\n",
        "from sklearn.model_selection import KFold\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "# --- 1. Download and Load Dataset ---\n",
        "# We'll use a small, publicly available sample to demonstrate.\n",
        "# This file is in the LibSVM format, common for LTR datasets.\n",
        "def download_data(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "def load_svmlight_file(file_path):\n",
        "    # This function reads the standard LibSVM format\n",
        "    data = []\n",
        "    qids = []\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' ')\n",
        "            relevance_score = int(parts[0])\n",
        "\n",
        "            # Extract query ID, handling potential float values in the string\n",
        "            query_id_str = parts[1].split(':')[1]\n",
        "            query_id = int(float(query_id_str)) # Convert to float first, then to int\n",
        "\n",
        "\n",
        "            # Features start from the third part\n",
        "            features = {int(p.split(':')[0]): float(p.split(':')[1]) for p in parts[2:]}\n",
        "\n",
        "            data.append((query_id, relevance_score, features))\n",
        "\n",
        "    # Convert to a DataFrame\n",
        "    df_data = []\n",
        "    for qid, rel_score, feats in data:\n",
        "        row = {'QueryId': qid, 'RelScore': rel_score}\n",
        "        row.update(feats)\n",
        "        df_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(df_data)\n",
        "    df = df.fillna(0) # Fill missing feature values with 0\n",
        "\n",
        "    # Sort by QueryId to ensure groups are contiguous\n",
        "    df = df.sort_values(by='QueryId').reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# URL for a small sample from the MQ2008 dataset\n",
        "data_url = \"https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/lambdarank/rank.train\"\n",
        "file_name = \"mq2008_sample.txt\"\n",
        "download_data(data_url, file_name)\n",
        "\n",
        "df = load_svmlight_file(file_name)\n",
        "\n",
        "# --- 2. Prepare Data for LGBMRanker ---\n",
        "# Exclude metadata columns from features\n",
        "X = df.drop(['QueryId', 'RelScore'], axis=1)\n",
        "y = df['RelScore']\n",
        "qids = df['QueryId']\n",
        "groups = df.groupby('QueryId').size().to_numpy()\n",
        "\n",
        "print(f\"Dataset loaded with {len(df)} documents across {len(groups)} queries.\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "\n",
        "# --- 3. Train the LGBMRanker Model with Cross-Validation ---\n",
        "# Using K-Fold for a robust evaluation\n",
        "k_fold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "models = []\n",
        "\n",
        "print(\"\\nStarting 3-fold cross-validation...\")\n",
        "\n",
        "for train_index, test_index in k_fold.split(X, y, groups=qids):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # LightGBM requires the `group` parameter to match the training data\n",
        "    groups_train = X_train.groupby(qids.iloc[train_index]).size().to_numpy()\n",
        "\n",
        "    ranker = LGBMRanker(\n",
        "        objective=\"lambdarank\",\n",
        "        metric=\"ndcg\",\n",
        "        n_estimators=50,\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "\n",
        "    ranker.fit(X_train, y_train, group=groups_train)\n",
        "    models.append(ranker)\n",
        "\n",
        "print(\"\\nModel training complete for all folds.\")\n",
        "\n",
        "# --- 4. Make Predictions for a New Query ---\n",
        "# In a real system, you would get new features for a query and use the best model.\n",
        "print(\"\\nMaking predictions on a new, hypothetical query...\")\n",
        "\n",
        "# Create a DataFrame for a new query with 3 documents.\n",
        "# The number of features must match the training data.\n",
        "num_features = X.shape[1]\n",
        "new_query_data = pd.DataFrame(np.random.rand(3, num_features), columns=X.columns)\n",
        "\n",
        "# Predict the relevance scores\n",
        "predictions = models[0].predict(new_query_data) # Use the first trained model\n",
        "new_query_data['PredictedScore'] = predictions\n",
        "ranked_results = new_query_data.sort_values(by='PredictedScore', ascending=False)\n",
        "\n",
        "print(\"\\nPredicted rankings for the new query:\")\n",
        "print(ranked_results[['PredictedScore']])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}